# -*- coding: utf-8 -*-
"""Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19uJBMazJfLyPw9zKaBy6sOsh5tRXve07

# Training regressors to predict epigenomic-metabolic interactions
**Author**: Scott Campit

# Summary
This notebook trains several regressors of varying complexity and assumptions to learn the relationships between GCPs and metabolomics. This is a complementary approach to analyzing the data as opposed to an optimization and factorization standpoint.

## Mount Google Drive to Colab
This bit of code mounts Drive to the Colab notebook, and writes in an accessory function that is needed to read in Google Sheets as Pandas dataframes.
"""

# Import necessary data science libraries
import pandas as pd
import numpy as np

# Load relevant libraries for Google Colab
#from google.colab import auth
#auth.authenticate_user()

# Allows us to read in Google Sheets via url
#import gspread
#from oauth2client.client import GoogleCredentials
#gc = gspread.authorize(GoogleCredentials.get_application_default())

# Mount Google Drive, which will allow you to read in files within your Google 
# Drive if you wish to repurpose this for other datasets
#from google.colab import drive
#drive.mount('/content/drive')

"""## Accessory functions
I use this code to read in various Google sheets as Pandas dataframes. So here's a function that simplifies this operation by a lot.
"""

def read_gsheet(url='', sheetname=''):
  """
  read_gsheet reads in a Google sheet via the shared url and sheetname, and 
  outputs a pandas dataframe.

  params:
    url:       A string containing the url to the Google sheet.
    sheetname: A string containing the sheet name to be read in.
|
  return:
    df:        A Pandas dataframe of the data set.

  """
  # Read in Google sheet data
  wb = gc.open_by_url(url)
  wks = wb.worksheet(sheetname)
  data = wks.get_all_values()

  # Construct dataframe with the first row as column names
  df = pd.DataFrame(data)
  header = df.iloc[0]
  df = df[1:]
  df.columns = header
  
  return df

def save_gsheet(df, url='', sheetname=''):
  """
  save_gsheet saves a dataframe to a Google sheet using a url and a specified
  sheetname.

  :params df:        A pandas dataframe.
  :params url:       A string of the url to save the pandas dataframe data in.
  :params sheetname: A string of the sheet.
  """
  
  gc = gspread.authorize(GoogleCredentials.get_application_default())
  wb = gc.open_by_url(url)
  wb = wb.add_worksheet(title=sheetname, 
                        rows=str(df.shape[0]), 
                        cols=str(df.shape[1]))
  set_with_dataframe(wb, df)

"""# 1. Univariate modeling without PCA 
This code block will train several univariate models for each target variable $Y_{i}$, given the input dataset $X$.

## 1a. Load Single Metabolomics and GCP datasets
This code block loads the GCP and metabolomics datasets for single histone markers and metabolites.
"""

# Needed for running on Google Colab
#gcp_url = 'https://docs.google.com/spreadsheets/d/1eRwYUZve16ALg-DvwAooWPvMJfRn8j6ggUp-HVDb84A/edit?usp=sharing'
#met_url = 'https://docs.google.com/spreadsheets/d/1V1JEugHtnQrfqOxFD-Nsqa3KM2PaegNsp22J4dp8vy4/edit?usp=sharing'

#GCP = read_gsheet(url=gcp_url, sheetname='ACME')
#MET = read_gsheet(met_url, sheetname='All + FA')

# Needed to run server side
gcp_path = '/nfs/turbo/umms-csriram/scampit/Data/Proteomics/CCLE/CCLE Global Chromatin Profiles.xlsx'
met_path = '/nfs/turbo/umms-csriram/scampit/Data/Metabolomics/CCLE/CCLE metabolomics dataset.xlsx'

GCP = pd.read_excel(gcp_path, 'ACME')
MET = pd.read_excel(met_path, 'All')

#print(GCP.shape)
#print(MET.shape)

"""Let's also get a preview of the datasets"""

#print(GCP.head(10))
#print(MET.head(10))

"""We don't need some of the meta data in the `MET` dataframe"""

MET = MET.drop(["Tissue", "Medium", "Culture"], axis=1)

"""## 1b. Get intersecting cancer cell lines between the two datasets
To preprocess the data, we'll do a couple of things, including:
  * Match by cell lines
  * Sort by index
  * Remove unncessary columns
  * Z-score the metabolomics data
"""

idx = list(set(GCP['Cell Line']) & set(MET['CCL']))
GCP = GCP[GCP['Cell Line'].isin(idx)]
MET = MET[MET['CCL'].isin(idx)]
GCP = GCP.drop_duplicates(subset='Cell Line', keep='first')
MET = MET.drop_duplicates(subset='CCL', keep='first')

# Must assert that cell name is string for some reason.
GCP['Cell Line'] = GCP['Cell Line'].astype(str)
MET['CCL'] = MET['CCL'].astype(str)

GCP = GCP.sort_values('Cell Line')
MET = MET.sort_values('CCL')


"""To sanity check, let's continue looking at the dataframes"""

#print(GCP.head(10))
#print(MET.head(10))

"""Finally, we'll save the cell line names in a list, and remove them from their respective data frames."""

cell_lines = list(GCP.pop('Cell Line'))
MET = MET.drop('CCL', axis=1)

"""## 1c. SANITY CHECK: Plot the data distributions for the metabolites and histone ratios
Let's check out the distributions between the two datasets. Note that for some reason the data is not being read as a numeric data type. Thus, I also need to coerce the data into a numeric data type.
"""

GCP = GCP.apply(pd.to_numeric, errors = 'coerce')
MET = MET.apply(pd.to_numeric, errors = 'coerce')

#import matplotlib.pyplot as plt
#import seaborn as sns
#plt.figure(figsize=(12, 12))
#sns.distplot(GCP, bins=100)
#sns.distplot(MET, bins=100)
#plt.legend(['GCP', 'Metabolomics'])

"""The only thing we'll probably need to do is mean center the metabolomics data."""

MET = MET - MET.mean()

#import matplotlib.pyplot as plt
#import seaborn as sns
#plt.figure(figsize=(12, 12))
#sns.distplot(GCP, bins=100)
#sns.distplot(MET, bins=100)
#plt.legend(['GCP', 'Metabolomics'])

"""## 1d. Train on GCP to predict metabolism
First, let's split the data into training and test sets.
"""

from sklearn.model_selection import train_test_split

"""Convert to Numpy array."""

GCP = GCP.to_numpy()
MET = MET.to_numpy()

"""Let's ensure all NaNs are 0."""

# Ensure all values are finite
GCP = np.nan_to_num(GCP, nan=0)
MET = np.nan_to_num(MET, nan=0)

"""Split the data into validation (30%) and training (70%) data."""

# Split the CCLE data into a validation set
Xtrain, Xval, Ytrain, Yval = train_test_split(
                                              GCP, MET, 
                                              test_size=0.3, random_state=0
)

"""Print $X_{train}$ and $Y_{train}$."""

import sys
np.set_printoptions(threshold=sys.maxsize)
#print(Xtrain)
#print(Ytrain)

"""Print shape of $X_{train}$ and $Y_{train}$."""

#print(Xtrain.shape)
#print(Ytrain.shape)
#print(Xval.shape)
#print(Yval.shape)

"""Ensure that all values are finite."""

#print(np.sum(Xtrain == np.inf))
#print(np.sum(Xtrain == -np.inf))
#print(np.sum(Xtrain == np.NaN))
#print(np.sum(Ytrain == np.inf))
#print(np.sum(Ytrain == -np.inf))
#print(np.sum(Ytrain == np.NaN))

"""### 1d.1. 3-fold cross validation for univariate regression
Now let's train a bunch of non-linear regressors and evaluate their performance.

We'll train the following regressors:
  * Ordinary least squares
  * Robust regression
  * Ridge
  * LASSO
  * Elastic Net
  * Support vector regressors
  * Random Forests
  * Extra Trees
  * Gradient boosting
  * XGBoost
"""

# ML models
#!pip install scikit-optimize
from sklearn import linear_model as lm
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.ensemble import GradientBoostingRegressor
#!pip install xgboost
import xgboost as xgb

# Accessory functions
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
from skopt import BayesSearchCV
from skopt.space import Real, Categorical, Integer

#!pip3 install progressbar
#from time import sleep
#import progressbar

# Suppress annoying warnings
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
from joblib import Parallel, delayed, parallel_backend
from skopt import dump, load
from skopt.utils import use_named_args
from sklearn.multioutput import MultiOutputRegressor

import multiprocessing

"""### 1d.2. Load some hyperparameters to sample
Let's define the search parameters we'll use for hyperparameter optimization
"""

# Robust 
robust_params = {
    'epsilon': Real(1, 10, "log-uniform"),
    'alpha': Real(10**-5, 10**0, "log-uniform")
}

# Ridge
ridge_params = {
    'alpha': Real(10**-5, 10**0, "log-uniform")
}

# LASSO
lasso_params = {
    'alpha': Real(10**-5, 10**0, "log-uniform")
}

# Elastic Net
en_params = {
    'alpha': Real(10**-5, 10**0, "log-uniform"),
    'l1_ratio': Real(10**-5, 10**0, "log-uniform")
}

# SVM
svm_params = {
    'kernel': {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'},
    'degree': Integer(1, 10),
    'gamma': Real(10**-5, 10**0, "log-uniform"),
    'C': Real(10**-5, 10**0, "log-uniform")
}

# Gradient boosting
gb_params = {
    'max_depth': Integer(1, 5),
    'max_features': Categorical(['auto', 'sqrt', 'log2']),
    'min_samples_split': Integer(2, 100),
    'min_samples_leaf': Integer(1, 100),
    'learning_rate': Real(10**-5, 10**0, "log-uniform")
}

# Random Forests
rf_params = {
    'max_depth': Integer(1, 5),
    'max_features': Integer(1, Xtrain.shape[1]-1),
    'min_samples_split': Integer(2, 100),
    'min_samples_leaf': Integer(1, 100)
}

# Extra Trees
et_params = {
    'max_depth': Integer(1, 5),
    'max_features': Integer(1, Xtrain.shape[1]-1),
    'min_samples_split': Integer(2, 100),
    'min_samples_leaf': Integer(1, 100),
}

# XGBoost
xgb_params ={
    'gamma': Integer(1, 10),
    'learning_rate': Real(10**-5, 0.99, prior="log-uniform"),
    'max_depth': Integer(3, 10),
    'reg_alpha': Real(10**-5, 1, prior="log-uniform"),
    'reg_lambda':Real(10**-5, 1, prior="log-uniform"),
    'max_delta_step': Integer(0, 10),
}

"""### 1d.3. Store everything in lists so we can iterate the code in a for loop
We'll run all steps as a single for loop. So we need to save the initial model structures, the hyperparameters, and the names of the pickled files in lists.
"""

models = [
          lm.HuberRegressor(max_iter=1000),
          lm.Ridge(),
          lm.Lasso(),
          lm.ElasticNet(),
          SVR(),
          RandomForestRegressor(),
          GradientBoostingRegressor(),
          ExtraTreesRegressor(),
          xgb.XGBRegressor()
]
params = [
          robust_params,
          ridge_params,
          lasso_params,
          en_params,
          svm_params,
          rf_params,
          gb_params,
          et_params,
          xgb_params
]

# Names stored on local
#names = [
#         '/home/scampit/Data/Models/GCP2Met/robust.pkl',
#         '/home/scampit/Data/Models/GCP2Met/ridge.pkl',
#         '/home/scampit/Data/Models/GCP2Met/lasso.pkl',
#         '/home/scampit/Data/Models/GCP2Met/en.pkl',
#         '/home/scampit/Data/Models/GCP2Met/svr.pkl',
#         '/home/scampit/Data/Models/GCP2Met/rf.pkl',
#         '/home/scampit/Data/Models/GCP2Met/gb.pkl',
#         '/home/scampit/Data/Models/GCP2Met/et.pkl',
#         '/home/scampit/Data/Models/GCP2Met/xgb.pkl'
#]

# Path for server
names = [
         '/nfs/turbo/umms-csriram/scampit/Data/Models/GCP2Met/robust.pkl',
         '/nfs/turbo/umms-csriram/scampit/Data/Models/GCP2Met/ridge.pkl',
         '/nfs/turbo/umms-csriram/scampit/Data/Models/GCP2Met/lasso.pkl',
         '/nfs/turbo/umms-csriram/scampit/Data/Models/GCP2Met/en.pkl',
         '/nfs/turbo/umms-csriram/scampit/Data/Models/GCP2Met/svr.pkl',
         '/nfs/turbo/umms-csriram/scampit/Data/Models/GCP2Met/rf.pkl',
         '/nfs/turbo/umms-csriram/scampit/Data/Models/GCP2Met/gb.pkl',
         '/nfs/turbo/umms-csriram/scampit/Data/Models/GCP2Met/et.pkl',
         '/nfs/turbo/umms-csriram/scampit/Data/Models/GCP2Met/xgb.pkl'
]

# Path for Google Drive
#names = [
#         '/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/GCP2MET/robust.pkl',
#         '/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/GCP2MET/ridge.pkl',
#         '/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/GCP2MET/lasso.pkl',
#         '/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/GCP2MET/en.pkl',
#         '/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/GCP2MET/svr.pkl',
#         '/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/GCP2MET/rf.pkl',
#         '/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/GCP2MET/gb.pkl',
#         '/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/GCP2MET/et.pkl',
#         '/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/GCP2MET/xgb.pkl'
#]

"""### 1d.4. Specify k-folds operator
We'll specify the number of k-folds.
"""

# Set the kfold operator to split 3 times with shuffle
kfold = KFold(n_splits=3, 
              shuffle=True, 
              random_state=0)

"""### 1d.5. Create a function that trains the models
We'll create a function that trains the model and performs the following steps:
  1. Define the BayesOpt object.
  2. Compute a model for each feature.
  3. Save all models based on a designated path.
"""

def train_models(models, params, Xtrain, Ytrain, kfold, filename):
  """
  train_models performs kfold bayesian hyperparameter tuning for different 
  models, and saves the output for model persistence.

  :param models: A single sklearn model object or list of sklearn model objects.
  :param params: A dictionary or list of dictionaries containing hyperparameters 
                to tune.
  :param Xtrain: A numpy array or pandas dataframe containing the training data.
  :param Ytrain: A numpy array or pandas dataframe containing the output data.
  :param kfold:  An integer or sklearn object determining the kfold operation 
                performed.
  :param filename: A string or list of paths to save the models (pickle).

  """
  no_of_cpus = multiprocessing.cpu_count()

  with parallel_backend('threading', n_jobs=no_of_cpus):
    for i in range(len(models)):
      opt = BayesSearchCV(
                          estimator=models[i],
                          search_spaces=params[i],
                          n_iter=30,
                          cv=kfold,
                          n_jobs=-1,
                          random_state=0
      )

      mdls =[]
      #bar.start()
      for j in range(Ytrain.shape[1]):
        _ = opt.fit(Xtrain, Ytrain[:, j])
        mdls.append(opt)
        dump(res=mdls, filename=filename[i])
        #bar.update(j)
        #sleep(0.1)
      #print("Finished hyperparameter optimization and cross validation for model number: " 
      #              + str(i))

"""Now let's train the models."""

train_models(models, params, Xtrain, Ytrain, kfold, names)

"""## 1e. Train on metabolomics to predict GCP
Now we'll do the reverse problem, using the same hyperparameters and functions set up above. The only other thing we'll change besides the $X$ and $Y$ variables are the file names to save these sets of models.
"""

# Split the CCLE data into a validation set
Xtrain, Xval, Ytrain, Yval = train_test_split(
                                              MET, GCP, 
                                              test_size=0.3, random_state=0
)

# Desktop version
#names = [
#         '/home/scampit/Data/Models/Met2GCP/robust.pkl',
#         '/home/scampit/Data/Models/Met2GCP/ridge.pkl',
#         '/home/scampit/Data/Models/Met2GCP/lasso.pkl',
#         '/home/scampit/Data/Models/Met2GCP/en.pkl',
#         '/home/scampit/Data/Models/Met2GCP/svr.pkl',
#         '/home/scampit/Data/Models/Met2GCP/rf.pkl',
#         '/home/scampit/Data/Models/Met2GCP/gb.pkl',
#         '/home/scampit/Data/Models/Met2GCP/et.pkl',
#         '/home/scampit/Data/Models/Met2GCP/xgb.pkl'
#]

# Path for server
names = [
         '/nfs/turbo/umms-csriram/scampit/Data/Models/Met2GCP/robust.pkl',
         '/nfs/turbo/umms-csriram/scampit/Data/Models/Met2GCP/ridge.pkl',
         '/nfs/turbo/umms-csriram/scampit/Data/Models/Met2GCP/lasso.pkl',
         '/nfs/turbo/umms-csriram/scampit/Data/Models/Met2GCP/en.pkl',
         '/nfs/turbo/umms-csriram/scampit/Data/Models/Met2GCP/svr.pkl',
         '/nfs/turbo/umms-csriram/scampit/Data/Models/Met2GCP/rf.pkl',
         '/nfs/turbo/umms-csriram/scampit/Data/Models/Met2GCP/gb.pkl',
         '/nfs/turbo/umms-csriram/scampit/Data/Models/Met2GCP/et.pkl',
         '/nfs/turbo/umms-csriram/scampit/Data/Models/Met2GCP/xgb.pkl'
]

# Google Drive version
#names = [
#         '/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/MET2GCP/robust.pkl',
#         '/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/MET2GCP/ridge.pkl',
#         '/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/MET2GCP/lasso.pkl',
#         '/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/MET2GCP/en.pkl',
#         '/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/MET2GCP/svr.pkl',
#         '/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/MET2GCP/rf.pkl',
#         '/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/MET2GCP/gb.pkl',
#         '/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/MET2GCP/et.pkl',
#         '/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/MET2GCP/xgb.pkl'
#]

train_models(models, params, Xtrain, Ytrain, kfold, names)

"""# 2. Evaluating the Univariate ML models without PCA
## 2.1 Evaluate the GCP to MET ML models
Now that I have some models trained up, it's time to create some data structures that will have the metrics I want. First, let's grab the validation set again from the `train_test_split()` function. Because the seed is set, it should get me the same entries.
"""

# Split the CCLE data into a validation set
Xtrain, Xval, Ytrain, Yval = train_test_split(
    GCP, MET, test_size=0.3, random_state=0
)

"""## 2.2 Load relevant evaluation libraries
Next, we'll load some libraries we'll be using to evaluate the predicted value against the true value.
"""

from scipy.stats import pearsonr
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error

"""## 2.3 Load GCP -> Met ML models
Let's now load the models we have trained to predict metabolite values from chromatin profiles.
"""

# Local Machine
#mdls = [load('/home/scampit/Data/Models/GCP2Met/robust.pkl'),
#         load('/home/scampit/Data/Models/GCP2Met/ridge.pkl'),
#         load('/home/scampit/Data/Models/GCP2Met/lasso.pkl'),
#         load('/home/scampit/Data/Models/GCP2Met/en.pkl'),
#         load('/home/scampit/Data/Models/GCP2Met/svr.pkl'),
#         load('/home/scampit/Data/Models/GCP2Met/rf.pkl'),
#         load('/home/scampit/Data/Models/GCP2Met/gb.pkl'),
#         load('/home/scampit/Data/Models/GCP2Met/et.pkl'),
#         load('/home/scampit/Data/Models/GCP2Met/xgb.pkl')
#]

# Server
mdls = [load('/nfs/turbo/umms-csriram/scampit/Data/Models/GCP2Met/robust.pkl'),
         load('/nfs/turbo/umms-csriram/scampit/Data/Models/GCP2Met/ridge.pkl'),
         load('/nfs/turbo/umms-csriram/scampit/Data/Models/GCP2Met/lasso.pkl'),
         load('/nfs/turbo/umms-csriram/scampit/Data/Models/GCP2Met/en.pkl'),
         load('/nfs/turbo/umms-csriram/scampit/Data/Models/GCP2Met/svr.pkl'),
         load('/nfs/turbo/umms-csriram/scampit/Data/Models/GCP2Met/rf.pkl'),
         load('/nfs/turbo/umms-csriram/scampit/Data/Models/GCP2Met/gb.pkl'),
         load('/nfs/turbo/umms-csriram/scampit/Data/Models/GCP2Met/et.pkl'),
         load('/nfs/turbo/umms-csriram/scampit/Data/Models/GCP2Met/xgb.pkl')
]

# Google Drive
#mdls = [load('/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/GCP2MET/robust.pkl'),
#         load('/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/GCP2MET/ridge.pkl'),
#         load('/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/GCP2MET/lasso.pkl'),
#         load('/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/GCP2MET/en.pkl'),
#         load('/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/GCP2MET/svr.pkl'),
#         load('/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/GCP2MET/rf.pkl'),
#         load('/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/GCP2MET/gb.pkl'),
#         load('/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/GCP2MET/et.pkl'),
#         load('/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/GCP2MET/xgb.pkl')
#]

"""## 2.4 Create a function that will evaluate the models
The `evaluate_models()` function will compute evaluation metrics and spit out the final metrics of interest.
"""

def evaluate_models(models, Xval, Yval):
  """
  evaluate_models returns metrics from the model predictions, include the pearson
  correlation coefficient, coefficient of determination, MSE, and MAE.

  :param models:         A scikit-learn model object or list of model objects.
  :param Xval:           A numpy array or pandas dataframe containing 
                         validation set input data.
  :param Yval:           A numpy array or pandas dataframe containing 
                         validation set output data.
  :return final_metrics: A pandas dataframe or list of dfs containing the final 
                         evaluation metrics
  """

  final_metrics = []
  for j in range(len(models)):
    # Iterate through model objects
    m = models[j]

    r_values = list()
    p_values = list()
    mse_values = list()
    mae_values = list()

    # Iterate through features
    for i in range(len(m)):
      mdl = m[i]
      ypred = mdl.predict(Xval)
      r, pvalue = pearsonr(ypred, Yval[:, i])
      mse = mean_squared_error(ypred, Yval[:, i])
      mae = mean_absolute_error(ypred, Yval[:, i])

      r_values.append(r)
      p_values.append(pvalue)
      mae.append(mae)
      mse.append(mse)

    # Save the metrics in a dataframe
    pre_df = {
              "Pearson": r_values, 
              "Pvalue":  p_values,
              "MSE":     mse,
              "MAE":     mae
              }
    df = pd.DataFrame(pre_df)
    final_metrics.append(df)

    return final_metrics

"""## 2.5 Evaluate the models
Then we'll perform the following operations:
  1. Concatenate the results into a single dataframe
  2. Append the metabolite names to the list
  3. Sort the values in ascending alphabetical order by metabolite name
  4. Save the final results to the Google Sheet.
"""

final_metrics = evaluate_models(mdls, Xval, Yval)

# Flatten the array so that 
final_metrics = pd.concat(final_metrics, axis=1)
final_metrics["Metabolites"] = metabolites
final_metrics = final_metrics.sort_values(by=["Metabolites"], 
                                          axis=1, 
                                          ascending=True)

# Save to Google Sheet 
#url = 'https://docs.google.com/spreadsheets/d/1_tFjeBplSfozCw0VIU84j8d0NTm4CyOAFr9tXQfBLoE/edit?usp=sharing'
#sheetname = 'GCP2Met'
#save_gsheet(final_metrics, url, sheetname)

# Save to local
#path = '/home/scampit/Data/Models/GCP2Met/gcp2met_metrics.csv'
#final_metrics.to_csv(path)

# Save to server
path = '/nfs/turbo/umms-csriram/scampit/Data/Models/GCP2Met/gcp2met_metrics.csv'
final_metrics.to_csv(path)

"""## 2.6 Evaluate the MET to GCP ML models
Now let's do the reverse using the same operations described above.
"""

# Split the CCLE data into a validation set
Xtrain, Xval, Ytrain, Yval = train_test_split(
    MET, GCP, test_size=0.3, random_state=0
)

# Local
#mdls = [load('/home/scampit/Data/Models/Met2GCP/robust.pkl'),
#        load('/home/scampit/Data/Models/Met2GCP/ridge.pkl'),
#        load('/home/scampit/Data/Models/Met2GCP/lasso.pkl'),
#        load('/home/scampit/Data/Models/Met2GCP/en.pkl'),
#        load('/home/scampit/Data/Models/Met2GCP/svr.pkl'),
#        load('/home/scampit/Data/Models/Met2GCP/rf.pkl'),
#        load('/home/scampit/Data/Models/Met2GCP/gb.pkl'),
#        load('/home/scampit/Data/Models/Met2GCP/et.pkl'),
#        load('/home/scampit/Data/Models/Met2GCP/xgb.pkl')
#]

# Server
mdls = [load('/nfs/turbo/umms-csriram/scampit/Data/Models/Met2GCP/robust.pkl'),
         load('/nfs/turbo/umms-csriram/scampit/Data/Models/Met2GCP/ridge.pkl'),
         load('/nfs/turbo/umms-csriram/scampit/Data/Models/Met2GCP/lasso.pkl'),
         load('/nfs/turbo/umms-csriram/scampit/Data/Models/Met2GCP/en.pkl'),
         load('/nfs/turbo/umms-csriram/scampit/Data/Models/Met2GCP/svr.pkl'),
         load('/nfs/turbo/umms-csriram/scampit/Data/Models/Met2GCP/rf.pkl'),
         load('/nfs/turbo/umms-csriram/scampit/Data/Models/Met2GCP/gb.pkl'),
         load('/nfs/turbo/umms-csriram/scampit/Data/Models/Met2GCP/et.pkl'),
         load('/nfs/turbo/umms-csriram/scampit/Data/Models/Met2GCP/xgb.pkl')
]

# Google Drive
#mdls = [load('/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/MET2GCP/robust.pkl'),
#        load('/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/MET2GCP/ridge.pkl'),
#        load('/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/MET2GCP/lasso.pkl'),
#        load('/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/MET2GCP/en.pkl'),
#        load('/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/MET2GCP/svr.pkl'),
#        load('/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/MET2GCP/rf.pkl'),
#        load('/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/MET2GCP/gb.pkl'),
#        load('/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/MET2GCP/et.pkl'),
#        load('/content/drive/My Drive/Work/Analysis/eGEMM/ML/Regression/MET2GCP/xgb.pkl')
#]
final_metrics = evaluate_models(mdls, Xval, Yval)

# Flatten the array so that 
final_metrics = pd.concat(final_metrics, axis=1)
final_metrics["GCP"] = gcps
final_metrics = final_metrics.sort_values(by=["GCP"], 
                                          axis=1, 
                                          ascending=True)

# Save the metrics to the Google sheet directly
#url = 'https://docs.google.com/spreadsheets/d/1_tFjeBplSfozCw0VIU84j8d0NTm4CyOAFr9tXQfBLoE/edit?usp=sharing'
#sheetname = 'Met2GCP'
#save_gsheet(final_metrics, url, sheetname)

# Save to local
#path = '/home/scampit/Data/Models/Met2GCP/met2gcp_metrics.csv'
#final_metrics.to_csv(path)

# Save to server
path = '/nfs/turbo/umms-csriram/scampit/Data/Models/Met2GCP/met2gcp_metrics.csv'
final_metrics.to_csv(path)
